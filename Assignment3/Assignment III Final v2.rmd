---
title: "ISOM 674 - Graded Assignment III"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---
## Team members
Nikhil Chalakkal (nikhil.chalakkal@emory.edu)  
Leo Ma (chengyao.ma@emory.edu)  
Ryan Chi (ryan.chi@emory.edu)  

## Preprocessing:

### Importing packages & Setting Directory

```{r}
rm(list = ls(all = TRUE))
setwd('C:/Users/cyma9/Dropbox/IntroML1/Assignment/Machine-Learning-I/Assignment3')
library(leaps)
library(dplyr)
library(randomForest)
library(caret)
```

### Reading and Processing Data

We input the coded data, the un-coded data and the order file.

```{r, warning = FALSE, echo = T, eval = T}
orig = read.csv('spambasedata-Orig.csv',header = TRUE)
coded = read.csv('spambasedata-Coded.csv',header = TRUE, stringsAsFactors = FALSE)
load(file = "SpamdataPermutation.RData")
```

We factored the categorical variables in the coded data into data that is represented by number, depending on the value of the categories as follows:

'Zero'  = 0  
'Low'   = 1  
'Med'   = 2  
'High'  = 3  
'A'     = 0  
'B'     = 1  
'C'     = 2  
'D'     = 3  
'E'     = 4  
'F'     = 5  

```{r, warning = FALSE, echo = T, eval = T}
coded_factor = coded
coded_factor[coded_factor == 'Zero'] = 0
coded_factor[coded_factor == 'Low'] = 1
coded_factor[coded_factor == 'Med'] = 2
coded_factor[coded_factor == 'High'] = 3
coded_factor[coded_factor == 'A'] = 0
coded_factor[coded_factor == 'B'] = 1
coded_factor[coded_factor == 'C'] = 2
coded_factor[coded_factor == 'D'] = 3
coded_factor[coded_factor == 'E'] = 4
coded_factor[coded_factor == 'F'] = 5
```

Once this is done, we use the order file to split both coded data and original uncoded data into training data and validation data - this is as per what was done in class. The training and validation data are split in a 60 : 40 ratio. 

```{r, warning = FALSE, echo = T, eval = T}
training_ord = ord[1:2761]
validation_ord = ord[2762:4601]

training_orig = orig[training_ord,]
validation_orig = orig[validation_ord,]

training_coded = coded_factor[training_ord,]
validation_coded = coded_factor[validation_ord,]

training_coded_uf = coded[training_ord,]
validation_coded_uf = coded[validation_ord,]

x_training_orig = model.matrix(IsSpam~.,training_orig)
y_training_orig = training_orig$IsSpam
x_validation_orig = model.matrix(IsSpam~.,validation_orig)
y_validation_orig = validation_orig$IsSpam

x_training_coded = model.matrix(IsSpam~., training_coded)
y_training_coded = training_coded$IsSpam
x_validation_coded = model.matrix(IsSpam~.,validation_coded)
y_validatoin_coded = validation_coded$IsSpam
```

## Defining Functions

We define a few functions (GetAUC, Bayes_Function, Prob_Function) which would be used later in the assignment.

### GetAUC function
This function returns the AUC when two vectors are passed into it. The two vectors are the vector of probabilities predicted by the model and the vector of 0's and 1's indicating the realized classifications of each observation.

```{r, warning = FALSE, echo = T, eval = T}
GetAUC <- function(Pvec,Cvec,Plot=F,Add=F) {
  NHam <- sum(Cvec==0)
  NSpam <- sum(Cvec==1)
  PvecS <- unique(sort(Pvec))
  x <- rep(NA,length(PvecS))
  y <- rep(NA,length(PvecS))
  for(i in 1:length(PvecS)) {
    x[i] <- sum(Pvec>=PvecS[i]&Cvec==0)/NHam
    y[i] <- sum(Pvec>=PvecS[i]&Cvec==1)/NSpam
  }
  x <- c(0,x,1)
  y <- c(0,y,1)
  ord <- order(x)
  x <- x[ord]
  y <- y[ord]
  
  AUC <- sum((x[2:length(x)]-x[1:(length(x)-1)])*(y[2:length(y)]+y[1:(length(y)-1)])/2)
  return(AUC)
}
```

### Bayes_Function
This function returns the probability of an event given a certain set of parameters. The function uses Bayes Theorem, and has four inputs passed to it, which includes a row of data giving values of the parameters so mentioned, and the probability that an event is spam or not, depending on the parameters.

```{r, warning = FALSE, echo = T, eval = T}
Bayes_Function <- function(DataRow, PGivenSpam, PGivenHam, PSpam) {
  t1 <- 1.0
  t2 <- 1.0
  for(x in names(DataRow)) {
    t1 <- t1 * PGivenSpam[[x]][DataRow[x]]
    t2 <- t2 * PGivenHam[[x]][DataRow[x]]
  }
  out <- t1 * PSpam / (t1 * PSpam + t2 * (1 - PSpam))
  return(out)
}
```

### Prob_Function
This function a table which gives normalized frequency of the various values given in the vector that is passed to it. This normalized frequency can be used as a probability for the values in the vector.

```{r, warning = FALSE, echo = T, eval = T}
Prob_Function <- function(x) {
  Probs <- table(x)
  Probs <- Probs / sum(Probs)
  return(Probs)
}
```

## Question 1:

Read questions 2 and 3 below. In finding the best 10 feature models, think about whether or not you want to build the models on the training data set and then test the "built" model on the validation data set or whether you want to use the validation data set to determine what the best variables are. Indicate the strategy you take and explain what the likely effect is on possible overfitting to the validation data set. Indicate the pros and cons of you decision and why you made the decision you did.

### Answer:

The method that we used for feature selection is **Forward Selection**. We begin with the null model with no parameters, and then using a loop, we fit different regressions across all variables, and add to the existing model the variable that gives us the lowest overall AUC. We continue this approach until we reach 10 parameters or the AUC doesn't increase any more. 

The advantages of this: Compute time is much lesser than having to run 10 loops of 54 variables each to find the actual best model. However, the forward selection algorithm is a greedy algorithm, that might include variables that perform well in the early stages but become meaningless later on as the number of parameters increase.

For the both the **Na?ve Bayes Model** and the **Logistic Regression Model**, we would be using the training set to find what the ten best variables are, and then testing these results on the validation set to see if there is a significant difference in performance between the training and the validation sets. 

There is obviously the danger of overfitting the model when we use more data to train it - however, we can determine if the model is overfitting depending on whether the AUC percentage goes up as we use more data (80% instead of 60%). If it remains constant, or does not vary, then we can conclude that overfitting is not likely to happen.

## Question 2:

Using the coded spam data set, find the best Na?ve Bayes model that you can find that uses no more than 10 features.

### Answer:

The code for the Naive Bayes modeling is given below. Since this code cycles through 10 cycles of 55 variables each to find the ten best variables which give the highest AUC, it takes some time to execute. There were also NA's seen in the probability generated for the training and the validation data sets, which were replaced by zeros to ensure they didn't interfere with the execution of the code.

```{r, warning = FALSE, echo = T, eval = T}
PSpam <- mean(training_coded_uf$IsSpam)
wh <- training_coded_uf[["IsSpam"]] == 1

PGivenSpam <- lapply(training_coded_uf[training_coded_uf$IsSpam == 1,], FUN = Prob_Function)
PGivenHam <- lapply(training_coded_uf[training_coded_uf$IsSpam != 1,], FUN = Prob_Function)

num_para = ncol(training_coded_uf) - 1
parameters = colnames(training_coded_uf)
performance = 0
selection = vector()

for (i in c(1:10))
{
  selection_best = selection
  performance_best = performance
  for (j in c(1:num_para))
  {
    selection_try = selection
    if (j %in% selection == FALSE)
    {
      selection_try = append(selection_try, j)
      train_test = as.data.frame(training_coded_uf[,selection_try])
      prob_train <- apply(train_test, 1, FUN = Bayes_Function, PGivenSpam, PGivenHam, PSpam)
      prob_train[is.na(prob_train)] = 0
      performance_try = GetAUC(prob_train, training_coded_uf[,"IsSpam"])
      if (performance_try > performance_best)
      {
        performance_best = performance_try
        selection_best = selection_try
      }
    }
  }
  if (performance_best > performance)
  {
    performance = performance_best
    selection = selection_best
  }
}

prob_final <- apply(validation_coded_uf[, selection], 1, FUN = Bayes_Function, PGivenSpam, PGivenHam, PSpam)
prob_final[is.na(prob_final)] = 0

performance_nb = GetAUC(prob_final, validation_coded_uf[,"IsSpam"])
model_nb = selection
```

The final results of this model are given below:

```{r, warning = FALSE, echo = F, eval = T}
cat('The best AUC for the Naive Bayes model is :\n', performance_nb)
cat('\n\nThe ten best features chosen by the Naive Bayes model are:\n')
print(parameters[model_nb])
```

## Question 3:

Using the coded spam data set, find the best Logistic Regression model that you can find that uses no more than 10 features.

### Answer:

The code for the Logistic Regression modeling is given below. The loop used by this model is conceptually the same as used before.

```{r, warning = FALSE, echo = T, eval = T}
parameters = colnames(orig)
selection = list()
num_para = ncol(orig) - 1
performance = 0
for (i in c(1:10))
{
  selection_best = selection
  performance_best = performance
  for (j in c(1:num_para))
  {
    selection_try = selection
    if (parameters[j] %in% selection == FALSE)
    {
      selection_try = append(selection_try,parameters[j])
      formula_try = as.formula(paste('IsSpam~',paste(selection_try,collapse = '+')))
      model_try= glm(formula_try,data = training_coded, family = binomial(link = 'logit'))    
      prob_try = predict(model_try, newdata = validation_coded,type = 'response')
      performance_try = GetAUC(prob_try,validation_coded$IsSpam)
      if (performance_try > performance_best)
      {
        performance_best = performance_try
        selection_best = selection_try
      }
    }
  }
  if (performance_best > performance)
  {
    performance = performance_best
    selection = selection_best
  }
  else
    break
}

performance_LR_coded = performance
model_LR_coded = as.formula(paste('IsSpam~',paste(selection,collapse = '+')))
```

The final results of this model are given below:

```{r, warning = FALSE, echo = F, eval = T}
cat('The best AUC for the Logistic Regression model (with coded data) is :\n', performance_LR_coded)
cat('\n\nThe model based on the ten best features chosen by the Logistic Regression model for the coded data is:\n')
print(model_LR_coded)
```

## Question 4:

Using the un-coded spam data set, find the best Logistic Regression model that you can find that uses no more than 10 features.

### Answer:

The code for the Logistic Regression modeling is given below. The loop used by this model is conceptually the same as used before.

```{r, warning = FALSE, echo = T, eval = T}
parameters = colnames(orig)
selection = list()
num_para = ncol(orig) - 1
performance = 0
for (i in c(1:10))
{
  selection_best = selection
  performance_best = performance
  for (j in c(1:num_para))
  {
    selection_try = selection
    if (parameters[j] %in% selection == FALSE)
    {
      selection_try = append(selection_try,parameters[j])
      formula_try = as.formula(paste('IsSpam~',paste(selection_try,collapse = '+')))
      model_try= glm(formula_try,data = training_orig, family = binomial(link = 'logit'))    
      prob_try = predict(model_try, newdata = validation_orig,type = 'response')
      performance_try = GetAUC(prob_try,validation_orig$IsSpam)
      if (performance_try > performance_best)
      {
        performance_best = performance_try
        selection_best = selection_try
      }
    }
  }
  if (performance_best > performance)
  {
    performance = performance_best
    selection = selection_best
  }
  else
    break
}

performance_LR_orig = performance
model_LR_orig = as.formula(paste('IsSpam~',paste(selection,collapse = '+')))
```

The final results of this model are given below:

```{r, warning = FALSE, echo = F, eval = T}
cat('The best AUC for the Logistic Regression model (with un-coded data) is :\n', performance_LR_orig)
cat('\n\nThe model based on the ten best features chosen by the Logistic Regression model for the coded data is:\n')
print(model_LR_orig)
```

## Question 5:

Of these three models which seems to be better? How much of an effect did coding the variables seem to have?

### Answer:

We got the following AUC results for the models tested:

*Na?ve Bayes (Coded Variables)*           : 96.70%  
*Logistic Regression (Coded Variables)*   : 97.37%  
*Logistic Regression (Uncoded Variables)* : 96.67%  

Based on the results, the **Logistic Regression Model for Coded Variables** gives the best results, but not by a lot. This would mean that there is not much difference in the way the three models are predicting the result based on the dataset provided. We also see that using more data to train does not really improve the AUC by too much.

Coding the variables has improved the prediction slightly - from **96.7% to 97.4%**. This could be because the range of values for the variables fed into the model were fixed over a smaller range, and not decimal numbers over a very large range.

For all the models, we also split the data into an 80% training set and 20% validation set to see if there was a significant difference in the performance if there was more data given to train the models. However, we did not see a significant change in performance (less then 0.5%). Hence, we can infer that none of the models are overfitting.

## Question 6:

Using one other technique that we have learned about, find the best model that uses no more than 10 features and compare its performance to the other models.

### Answer:

We chose the **Random Forest** technique for this part. This function is built in the *randomforest* package. The code for the Random Forest modeling is given below.

```{r, warning = FALSE, echo = T, eval = T}
set.seed(1)
bag.ori = randomForest(IsSpam~.,data = coded_factor, subset = training_ord, mtry = 8, importance = TRUE)
ori_result = as.data.frame(bag.ori$importance)
ori_result$features = rownames(ori_result)
ori_result_sorted = arrange(ori_result, desc(IncNodePurity))
top_purity <- training_coded[,ori_result_sorted[1:10,"features"]]
purity_rf = randomForest(x = top_purity, y = y_training_coded, tree = 501, importance = TRUE)
rf1_prob = predict(purity_rf, newdata = validation_coded[,ori_result_sorted[1:10,"features"]],type = 'response')

model_rf = purity_rf
performance_rf = GetAUC(rf1_prob, validation_coded$IsSpam)
```

The results of this model are given below:

```{r, warning = FALSE, echo = F, eval = T}
cat('The best AUC for the Random Forest technique is :\n', performance_rf)
cat('\n\nThe ten best features chosen by the Random Forest technique are:\n')
print(colnames(top_purity))
```

## Question 7

Read Chapter 17 in Alpayden. This chapter discusses ensemble methods (Combining Multiple Learners). You will see that this can become quite complex.

## Question 8

Construct the best ensemble approach that you can based on the na?ve Bayes approach, one of the two logistic regression approaches, and the additional approach you selected for item (6). Base your ensemble approach on combining the probabilities from the models. Keep things very simple. How does the performance of the ensemble approach compare to the performance of the individual models?

### Answer

Based on the question, we created an **Ensemble Model** using the models made on the coded data - the Naive Bayes Model the Logistic Regression Model and the Random Forest. 

There are several methods using which we could model an ensemble. The simplest way was to average the predictions from the three models mentioned, and use the averaged probability for prediction. Another simple method was to take the majority result predicted by the three models as the final prediction.

Another way, which is a little more complex is to use the predictions (over the training data) from the three models mentioned as input variables and training another model to give us a final prediction over the validation data. The model used could be either a generalized linear model or a random forest, or any other method.

We used a two methods to get the ensemble result, so we could compare and see what the best result that we could achieve was. The methods used to ensemble were
1. Simple average of three model results.
2. Generalized Linear Model (glm) of the three results.

The code for the ensemble is given below:
   
```{r, warning = FALSE, echo = T, eval = T}
model_LR_coded_run= glm(model_LR_coded,data = training_coded, family = binomial(link = 'logit'))    

training_esb = training_orig
training_esb$nb <- apply(training_coded_uf[, model_nb], 1, FUN = Bayes_Function, PGivenSpam, PGivenHam, PSpam)
training_esb$nb[is.na(training_esb$nb)] = 0
training_esb$lr = predict(model_LR_coded_run, newdata = training_coded,type = 'response')
training_esb$rf = predict(model_rf,newdata = training_coded[,ori_result_sorted[1:10,"features"]],type = 'response')

validation_esb = validation_orig
validation_esb$nb <- apply(validation_coded_uf[, model_nb], 1, FUN = Bayes_Function, PGivenSpam, PGivenHam, PSpam)
validation_esb$lr = predict(model_LR_coded_run, newdata = validation_coded,type = 'response')
validation_esb$rf = predict(model_rf,newdata = validation_coded[,ori_result_sorted[1:10,"features"]],type = 'response')
validation_esb$mean = rowMeans(data.frame(validation_esb$nb, validation_esb$lr, validation_esb$rf), na.rm = TRUE)
validation_esb$nb[is.na(validation_esb$nb)] = 0

predictors_top <- c('nb','rf','lr') 
fitControl <- trainControl(method = "cv", number = 5, savePredictions = 'final', classProbs = T)

final_gbm = train(training_esb[,predictors_top],training_esb$IsSpam,method = 'rf', trControl = fitControl, tuneLength = 3)
esb_prob = predict(final_gbm, validation_esb[,predictors_top])

AUC_esb_mean <- GetAUC(validation_esb$mean, validation_coded$IsSpam)
AUC_esb_prob <- GetAUC(esb_prob, validation_coded$IsSpam)
```

The results of the ensemble are given below:

```{r, warning = FALSE, echo = F, eval = T}
cat('The AUC for the Ensemble using Simple Average is : ', AUC_esb_mean)
cat('\nThe AUC for the Ensemble using Simple Average is : ', AUC_esb_prob)
```

From the results, it is evident that the ensemble model gives us the best results as compared to the other three models considered. This could be because the ensemble takes the results of the other three models in question and applies a function to them to get the best result based on all three models.

Based on this exploration, seee that the absolute best result is obtained when we use the simplest ensembling function - the average.
